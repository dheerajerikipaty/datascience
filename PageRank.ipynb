{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank as a linear algebra problem\n",
    "### Introduction\n",
    "\n",
    "Modern search engines employ methods of ranking the results to provide the \"best\" results first that are more elaborate than just plain text ranking. One of the most known and influential algorithms for computing the relevance of web pages is the Page Rank algorithm used by the Google search engine. It was invented by Larry Page and Sergey Brin while they were graduate students at Stanford, and it became a Google trademark in 1998. The idea that Page Rank brought up was that, the importance of any web page can be judged by looking at the pages that link to it. If we create a web page i and include a hyperlink to the web page j, this means that we consider j important and relevant for our topic. If there are a lot of pages that link to j, this means that the common belief is that page j is important. If on the other hand, j has only one backlink, but that comes from an authoritative site k, (like www.google.com, www.cnn.com) we say that k transfers its authority to j; in other words, k asserts that j is important. Whether we talk about popularity or authority, we can iteratively assign a rank to each web page, based on the ranks of the pages that point to it. \n",
    "To this aim, we begin by picturing the Web net as a directed graph, with nodes represented by web pages and edges represented by the links between them.\n",
    "\n",
    "Suppose for instance, that we have a small Internet consisting of just 4 web sites www.page1.com, www.page2.com, www.page3.com, www.page4.com, referencing each other in the manner suggested by the picture below.\n",
    "\n",
    "In our model, each page should transfer evenly its importance to the pages that it links to. Node 1 has 3 outgoing edges, so it will pass on  of its importance to each of the other 3 nodes. Node 3 has only one outgoing edge, so it will pass on all of its importance to node 1. In general, if a node has k outgoing edges, it will pass on  of its importance to each of the nodes that it links to. Let us better visualize the process by assigning weights to each edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MiniInternet](images\\miniinternet1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us denote by A the transition matrix of the graph, $$ A = \\begin{bmatrix}\n",
    "0 & 0 & 1 & 1/2  \\\\\n",
    "1/3 & 0 & 0 & 0  \\\\\n",
    "1/3 & 1/2 & 0 & 1/2 \\\\\n",
    "1/3 & 1/2 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The above matrix is nothing but the importance transferred from one website to another in our network represented in a matrix form. Imagine the nodes 1,2,3 and 4 on above each of the rows and columns.\n",
    "\n",
    "What does each row in our matrix represent?\n",
    "The first row of the above matrix for example represents the importance transferred to website 1 by all other websites 2,3,4 in our mini internet. So each row represents the importance transfered to that website by all the other websites in the network.\n",
    "\n",
    "What does each column in our matrix represent?\n",
    "Each column represents the probability of a surfer landing on a website via the website on that column. confusing? For example, the first column tells you the probability of a surfer landing on any other website via website 1. Which means the value 0 in the first rw first column says that the probability of a surfer landing on site 1 via site 1 is 0. Same way 1/3 on column 1 row 2 says that the probability of a surfer landing on site 2 via site 1 is onethird (1/3) and so on. And if you observe each column adds up to 1. Just a fun fact this matrix is a column-stochastic matrix. A matrix is called column-stochastic if all of its entries are greater or equal to zero (nonnegative) and the sum of the entries in each column is equal to 1.Any column-stochastic matrix has z = 1 as eigenvalue. Why the eigenvalue is important? You will know soon.\n",
    "\n",
    "Let us denote by r1, r2, r3, and r4 the importance/ranks of the four pages. Analyzing the situation at each node we get the below system of equations:\n",
    "\n",
    "r1 = 0.r1 + 0.r2 +1.r3 + ½ r4 <br />\n",
    "r2 = 1/3.r1 + 0.r2 + 0.r3 + 0.r4 <br />\n",
    "r3 = 1/3.r1 + ½.r2 + 0.r3+ ½.r4 <br />\n",
    "r4 = 1/3.r1 + ½.r2 + 0.r3 + 0.r4 <br />\n",
    "\n",
    "The above set of equations can be written in the below matrix vector multiplication form.\n",
    "\n",
    "$$ \\begin{bmatrix}r1 \\\\r2\\\\r3 \\\\r4 \\\\\\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 1 & 1/2  \\\\1/3 & 0 & 0 & 0  \\\\1/3 & 1/2 & 0 & 1/2 \\\\1/3 & 1/2 & 0 & 0 \\\\\\end{bmatrix} \\begin{bmatrix}r1 \\\\r2\\\\r3 \\\\r4 \\\\\\end{bmatrix}$$\n",
    "\n",
    "Now the above can also be written as \n",
    "$$\\begin{bmatrix} 0 & 0 & 1 & 1/2  \\\\1/3 & 0 & 0 & 0  \\\\1/3 & 1/2 & 0 & 1/2 \\\\1/3 & 1/2 & 0 & 0 \\\\\\end{bmatrix}\\begin{bmatrix}r1 \\\\r2\\\\r3 \\\\r4 \\\\\\end{bmatrix} = 1. \\begin{bmatrix}r1 \\\\r2\\\\r3 \\\\r4 \\\\\\end{bmatrix}$$\n",
    "\n",
    "Does this look familiar? This is nothing but we are trying to find a eigenvector of a matrix transformation whose eigenvalue is 1. In other words when this matrix transforms our space we are trying to find that one vector which stays exactly the same even after the transformation or scales by 1 which is nothing but the same vector.\n",
    "Now how do we find this? If you can visualize this, lets first take any random vector in space belonging to the same dimension. In our example python code we take this as a 4X1 vector with 1/4 as the values since there are 4 websites/nodes.\n",
    "Now if we apply this transformation to the random vector, we will get a different vector post transformation. If we keep applying this transformation to this vector at some point we will hit the vector which will not change in value. Ie we will hit the eigenvector for this transformation.\n",
    "\n",
    "The intuition here is that when you perform repeated multiplication by the same matrix, you’re really repeatedly scaling the coefficients corresponding to the eigenvector basis by the size of the eigenvalue. When you do this a large number of times, the largest eigenvalue/eigenvector will come to dominate.\n",
    "\n",
    "Pretty simple, eigen vector by definition is a vector that will stay itself (same direction, but may have different length) after performing matrix transformation.\n",
    "So applying a matrix many times, will keep transforming, rotating, shrinking and expanding the vector till at some stage it will become one of the eigenvectors, then it will not change any more (change in direction). The number of iterations that this will take to find the eigenvector, we call this in our program \"iterations to convergence\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate solution (Just for fun and more eigentheory)\n",
    "Another solution is to solve the equations by substitution. But again this is something that is not very scalable as the number of websites in our network increase.\n",
    "![Equationsolve](images\\eqnsolve.JPG)\n",
    "\n",
    "Once we solve this we get one eigenvector. But that does not mean this vector’s eigenvalue is 1. But if you observe for any value of x1 we are just scaling the vector here on the vector’s span. So the proportions between the values x1 x2 x3 and x4 here do NOT change. Bcos when we multiply a scalar to this vector it does not matter what scalar we are multiplying here since we will multiply it to all of the entries in the vector. So eventhough this vector is not the eigenvector with eigenvalue 1, we can just use this vector as the rank for each of our websites. So what does this mean? Can we use any eigenvectors of the matrix transformation? Yes. That is because an eigenvector is nothing but its scaling by a scalar due to the transformation. That means that the vector itself is not changing. In other words it’s the same scaled version of the same vector. And since the vector is not changing scaling it means all components of the vector are scaled “in proportion”. So we will get the rank from ANY eigenvector of the matrix transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python to implement PageRank algorithm using Eigentheory solution\n",
    "Here L is the matrix that represents a network of a mini internet with 6 websites/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Before we begin, let's load the libraries.\n",
    "%pylab notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRank(linkMatrix, d) :\n",
    "    n = linkMatrix.shape[0]\n",
    "    M = d * linkMatrix + ((1-d)/n) * np.ones([n,n]) #Factoring in our damping parameter\n",
    "    r = np.ones(n) / n # Sets up the rank vector with initial values of 1/4\n",
    "    print (\"Initial Rank vector\") \n",
    "    print (r)\n",
    "    finalR = r\n",
    "    r = M @ r #Matrix vector multiplication\n",
    "    i = 0\n",
    "    #The below loop will execute until the resultant rank vector is an eigenvector or the difference between the \n",
    "    #resultant vector to the new vector is negligible\n",
    "    while la.norm(finalR - r) > 0.01 : \n",
    "        finalR = r\n",
    "        r = M @ r\n",
    "        i += 1\n",
    "    print(str(i) + \" iterations to convergence\")\n",
    "    print (\"Final Rank Vector/Eigenvector\")\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our internet with 7 websites represented in a matrix form\n",
    "L = np.array([[0,   0,  1, 1/2 ],\n",
    "              [1/3, 0,   0, 0 ],\n",
    "              [1/3, 1/2, 0, 1/2],\n",
    "              [1/3, 1/2, 0, 0 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Rank vector\n",
      "[0.25 0.25 0.25 0.25]\n",
      "6 iterations to convergence\n",
      "Final Rank Vector/Eigenvector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.38975694, 0.12731481, 0.29050926, 0.19241898])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageRank(L, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damping Parameter\n",
    "If you have observed the above code, we just did not mulitply the matrix L that represents our mini internet with the initial rank vector. We used a formula to calculate the matrix M from this matrix L.\n",
    "Let's consider an extension to our micro-internet where things start to go wrong.\n",
    "Say a new website is added to the micro-internet: 5.\n",
    "\n",
    "This website 5 is linked to by 4 and only links to itself. \n",
    "If we just use the transformation matrix directly the website 5 will be taking all the traffic on the micro-internet, and somehow coming at the top of the PageRank.\n",
    "This behaviour can be understood, because once a user get's to Website 5, they can't leave, as all links head back to 5.\n",
    "\n",
    "To combat this, we can add a small probability that the users don't follow any link on a webpage, but instead visit a website on the micro-internet at random.\n",
    "We'll say the probability of them following a link is $d$ and the probability of choosing a random website is therefore $1-d$.\n",
    "We can use a new matrix to work out where the Pat's visit each minute.\n",
    "$$ M = d \\, L + \\frac{1-d}{n} \\, J $$\n",
    "where $J$ is an $n\\times n$ matrix where every element is one.\n",
    "\n",
    "If $d$ is one, we have the case we had previously, whereas if $d$ is zero, we will always visit a random webpage and therefore all webpages will be equally likely and equally ranked.\n",
    "For this extension to work best, $1-d$ should be somewhat small - though we won't go into a discussion about exactly how small.\n",
    "\n",
    "This is the reason why we have the below piece of code in our script\n",
    "M = d * linkMatrix + ((1-d)/n) * np.ones([n,n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html\n",
    "http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture1/lecture1.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
